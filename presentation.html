<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			    <section data-markdown>
					<textarea data-template>
					  #### Orchestrating AI: Using Advanced Machine Learning Techniques to Automate Music Generation
					  <p class="fragment fade-in-then-out" style="font-size: 18px">Adrian Buchholz - 210417616</p>
					</textarea>
				</section>
				
				<!-- Project Overview -->
				<section>
					<section>
						<h2>Project Overview</h2>
					</section>
					<section>
						<h2>Problem</h2>
						<div class="container fragment zoom-in">
							<div class="image">
								<img src="https://cdn-icons-png.flaticon.com/512/14781/14781952.png " alt="Problem" style="width: 30%; float: left; filter: invert(100%);">
							</div>
							<div class="text" style="padding-top: 72px;">
								<p style="font-weight: bold; margin:0; display:inline; float:left; padding-right: 6px; font-size: 32px;">Music production</p>
								<ul style="font-size: 22px; width: 600px;">
									<li>is complex and time-consuming.</li>
									<li>is inaccessible to beginners.</li>
									<li>is expensive.</li>
									<li>requires a lot of knowledge and expertise</li>
								</ul>
							</div>
						</div>
					</section>
					<section>
						<h2>Solution</h2>
						<p class="fragment fade-in" style="font-size: 30px">Similar to other industries, the musical sector can leverage technological innovation and the latest advancements in AI to address:</p>
						<ul class=fragment fade-in" style="font-size: 22px">
							<li>Automated Composition - AI can analyse vast amounts of music to produce similar pieces.</li>
							<li>Accessible Music Creation Tools - AI can help those new to the industry.</li>
							<li>Lower Production Costs - AI can lower the need for expensive equipment.</li>
						</ul>
					</section>
					<section>
						<h2>Project Summary</h2>
						<div style="display:flex;">
							<blockquote class="fragment fade-in" style="width:auto; margin: 12px; padding: 24px; font-size: 30px;">
								The project will focus on the development of a music generation system that leverages advanced machine learning techniques to automate the music production process; with a focus on classical cantatas. 
							</blockquote>
						</div>
						<h6 class="fragment fade-in" style="font-size: 32px;">RNN Models • GAN Models • Transformer Models</h6>
					</section>
				</section>

				<!--  Aims & Objectives -->
				<section>
					<section>
						<h2>Aims & Objectives</h2>
					</section>
					<section data-auto-animate>
						<h2>Aims & Objectives</h2>
						<ul style="font-size: 22px; list-style: none;">
							<li><u>Objective #1:</u> Create a RNN that can “accurately” predict the next note in a sequence.</li>
							<li><u>Objective #2:</u> Experiment with different normalisation methods for the RNN model (Chords and Notes).</li>
							<li><u>Objective #3:</u> Create a GAN using a generator and discriminator that “compete” against one another to create music.</li>
							<li><u>Objective #4:</u> Modify the GAN model in "Objective #3" to work with multiple instruments and genres of music.</li>
							<li><u>Objective #5:</u> Create a Transformer model that can generate music using sequence-to-sequence techniques.</li>
						</ul>
					</section>
					<section data-auto-animate>
						<h2>Aims & Objectives</h2>
						<ul style="font-size: 22px; list-style: none; display: inline-block;" id="objectivesList">
							<div style="display: flex; align-items: center;">
								<span style="margin-right: 8px;"><i style="color: rgba(160, 160, 160, 0.872);" class="fa-solid fa-wrench fa-fw"></i></span>
								<li><u>Objective #4:</u> Modify the GAN model in "Objective #3" to work with multiple instruments and genres of music.</li>
							</div>
							<div style="display: flex; align-items: center;">
								<span style="margin-right: 8px;"><i style="color: green;"class="fa-solid fa-check fa-fw"></i></span>
								<li><u>Objective #5:</u> Create a Transformer model that can generate music using sequence-to-sequence techniques.</li>
							</div>
							<div style="display: flex; align-items: center;">
								<span style="margin-right: 8px;"><i style="color: green;"class="fa-solid fa-check fa-fw"></i></span>
								<li><u>Objective #3:</u> Create a GAN using a generator and discriminator that “compete” against one another to create music.</li>
							</div>
							<div style="display: flex; align-items: center;">
								<span style="margin-right: 8px;"><i style="color: green;" class="fa-solid fa-check fa-fw"></i></span>
								<li><u>Objective #2:</u> Experiment with different normalisation methods for the RNN model (Chords and Notes).</li>
							</div>
							<div style="display: flex; align-items: center;">
								<span style="margin-right: 8px;"><i style="color: green;" class="fa-solid fa-check fa-fw"></i></span>
								<li><u>Objective #1:</u> Create a RNN that can “accurately” predict the next note in a sequence.</li>
							</div>
						</ul>
					</section>
				</section>

				<!-- Key Technolgies -->
				<section>
					<section>
						<h3>Key Technologies/Libraries</h3>
						<div class="r-stack">
							<img class="fragment fade-in-then-out" src="https://cdn.freebiesupply.com/logos/large/2x/python-5-logo-png-transparent.png" alt="Python" style="width: 30%">
							<img class="fragment fade-in-then-out" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/1200px-TensorFlow_logo.svg.png" alt="Tensorflow" style="width: 50%">
							<img class="fragment fade-in-then-out" src="https://keras.io/img/logo.png" alt="Keras" style="width: 50%">
							<img class="fragment fade-in-then-out" src="https://static.thenounproject.com/png/374559-200.png" style="filter: invert(100%);" alt="Pretty MIDI" style="width: 30%">
						</div>
					</section>
					<section>
						<h3>Why these Technolgies?</h3>
						<div class="fragment zoom-in">
							<div style="width: 100%;">
								<blockquote style="width:auto; margin: 12px; padding: 24px; font-size: 26px; text-align: left;">
									I selected these technologies because of their versatility, scalability, performance, and large community following. <br><br>
									- Keras was utilised in CSC3831.<br>
									- MIDI is a standard format for music files and has several Python libraries for normalising data.<br>
								</blockquote>
							</div>
							<img src="https://cdn.freebiesupply.com/logos/large/2x/python-5-logo-png-transparent.png" alt="Python" style="width: 10%">
							<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/1200px-TensorFlow_logo.svg.png" alt="Tensorflow" style="margin-top: 48px; width: 20%">
							<img src="https://keras.io/img/logo.png" alt="Keras" style="width: 20%; margin-bottom: 48px;">
							<img src="https://static.thenounproject.com/png/374559-200.png" style="width: 10%; filter: invert(100%);" alt="Pretty MIDI">
						</div>
					</section>
					<section>
						<h3>How do MIDI files work?</h3>
						<div class="fragment zoom-in">
							<div style="width: 100%;">
								<blockquote style="width:auto; margin: 12px; padding: 24px; font-size: 26px; text-align: left;">
									- MIDI files do not contain the recorded sound but rather the instructions on what note to play and when. <br>
									- MIDI files contain header and track chunks. <br>
									- MIDI files are made up of "Note On" and "Note Off" events.
								</blockquote>
							</div>
						</div>
					</section>
				</section>
				
				<!-- Literature Review/Sources -->
				<section>
					<h3>Literature Review</h3>
					<div class="r-stack">
						<blockquote class="fragment zoom-in" style="width:auto; margin: 12px; padding: 24px; font-size: 26px;">
							There are many papers that have been written on the subject of music generation using machine learning. The most notable papers to my project are: 
							<ul class="fragment fade-in" ="font-size: 20px;">
								<li><a href="https://www.tensorflow.org/tutorials/audio/music_generation#setup">Generate Music with a RNN [1] </a> (RNN-Pitch Model)</li>
								<li><a href="https://research.google/pubs/music-transformer-generating-music-with-long-term-structure/ ">Music Transformer: Generating Music with Long-Term Structure [2]</a> (Transformer Model)</li>
								<li><a href="https://salu133445.github.io/musegan/pdf/musegan-aaai2018-paper.pdf">MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment [3]</a> (GAN and Multi-Track GAN Model)</li>
							</ul>
						</blockquote>
					</div>
				</section>

				<!-- How was Work Managed? -->
				<section>
					<h3>How was Work Managed?</h3>
					<div class="container fragment zoom-in">
						<div class="image">
							<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/2_waterfall%20methodology.png" alt="Waterfall Model" style="width: 40%; height: 40%; float: left; filter: invert(100%);">
						</div>
						<div class="text">
							  <p style="font-size: 26px; text-align: left; padding-left: 425px; padding-top:65px;">
								The project follows the <strong>Waterfall Model</strong>: <br>
								1. Research and collecting requirements<br>
								2. Model design and coding<br>
								3. Model finalization via testing and tweaking
							</p>
						</div>
					</div>
				</section>

				<!-- Progress & Ethical Considerations -->
				<section>
					<section>
						<h2>Overview of Progress</h2>
					</section>
					
					<!-- RNN PITCH -->
					<section>
						<h3>RNN-Pitch Model</h3>
						<p style="font-size: 22px">The RNN-Pitch Model works by estimating three values: pitch, duration, and step.</p>
						<img style="filter: invert(100);" class="fragment zoom-in" src="static/rnn-pitch.png">
					</section>
					<section>
						<h4>RNN-Pitch Model (Pitch, Duration, and Step)</h4>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape style="font-size: 18px;">
								[[pitch, step, duration], [60, 0.25, 0.5], [67, 0.25, 0.5], [69, 0.15, 0.5], ...]
							</code>
						</pre>
						<i class="fas fa-arrow-down fragment zoom-in"></i>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape style="font-size: 18px;">
								[61, 0, 0.5]
							</code>
						</pre>
						<p class="fragment zoom-in" style="font-size: 22px;">The primary issue with the RNN-Pitch Model was that it was unable to accurately predict timing intervals between notes (step and duration).</p>
					</section>
					<section>
						<h3>RNN-Pitch Model Example</h3>
						<audio controls src="static/rnn-pitch.mp3" type="audio/mpeg"></audio>
					</section>

					<!-- RNN CHORD -->
					<section>
						<h3>RNN-Chord Model</h3>
						<p style="font-size: 22px">The RNN-Chord Model fixed the flaws in the RNN-Pitch model (timing intervals) by converting song data into chords and notes.</p>
						<img style="filter: invert(100);" class="fragment zoom-in" src="static/rnn-chord.png">
					</section>
					
					<section data-auto-animate>
						<h4>RNN-Chord Model (Notes and Chords)</h4>
						<p style="font-size: 22px">How are chords represented as they are multiple notes played at the same time?</p>
						<pre>
							<code data-trim data-noescape>
								['B4', 'A4', 'A2', 'E3', '0.4', '10.0', '0.5', '0.4.7', ...]
							</code>
						</pre>
					</section>
					<section data-auto-animate>
						<h4>RNN-Chord Model (Notes and Chords)</h4>
						<p style="font-size: 22px">Notes are represented in the "Standard Note Name" format.</p>
						<pre>
							<code data-trim data-noescape>
								['B4', 'A4', 'A2', 'E3', ...]
							</code>
						</pre>
						<img style="margin: 0;" src="https://www.researchgate.net/publication/283460243/figure/fig8/AS:614346480685058@1523483023512/88-notes-classical-keyboard-Note-names-and-MIDI-numbers.png">
						<p style="font-size:14px; margin: 0; "><em>Retrieved From: <a href="https://www.researchgate.net/figure/88-notes-classical-keyboard-Note-names-and-MIDI-numbers_fig8_283460243">Mickaël Tits [7]</a></em></p>
					</section>
					<section data-auto-animate>
						<h4>RNN-Chord Model (Notes and Chords)</h4>
						<p style="font-size: 22px">Chords utilise <a href="https://web.mit.edu/music21/doc/moduleReference/moduleChord.html">"Pitch Class"</a> (0-11) to represent notes. This ensures that the model only has to consider 0-11 and not all 88 possibles notes and their permutations.</p>
						<pre>
							<code data-trim data-noescape>
								[..., '0.4', '10.0', '0.5', '0.4.7', ...]
							</code>
						</pre>
						<img style="margin: 0; filter: invert(100);" src="static/pitchclass.png" height="300">
						<p style="font-size:14px; margin: 0; "><em>Retrieved From: <a href="https://davidkulma.com/musictheory/integers">David Kulma [6]</a></em></p>
					</section>

					<section>
						<h3>RNN-Chord Model Example</h3>
						<audio controls src="static/rnn-chord.mp3" type="audio/mpeg"></audio>
					</section>

					<!-- GAN -->
					<section>
						<h3>GAN Model</h3>
						<p style="font-size: 22px">MIDI Data was normalised in three different ways: Notes and Chords, Frequency, and Note Events. The different normalisation techniques were attempted as training GAN Models can lead to issues such as mode collapse and tend to be finicky.</p>
						<img style="filter: invert(100);" class="fragment zoom-in" src="static/gan.png">
					</section>
					<section>
						<h5>GAN Model (Notes/Chords & Frequency)</h5>
						<p style="font-size: 22px">Notes and Chords utilised the same principles as in the RNN-Chord Model. <br> Frequency converted notes into their associated frequency.</p>
						<div class="fragment fade-in">
							<img style="margin: 0;" src="https://sengpielaudio.com/KeyboardAndFrequencies.gif">
							<p style="font-size:14px; margin: 0; "><em>Retrieved From: <a href="https://sengpielaudio.com/calculator-notenames.htm">Tontechnik-Rechner [5]</a></em></p>
						</div>
					</section>
					<section>
						<h3>GAN Model (Note Events)</h3>
						<p style="font-size: 22px">MIDI Files Consist of Note Events (NOTE ON and NOTE OFF).</p>
						<div class="fragment fade-in">
							<img style="margin: 0;" src="https://www.audiolabs-erlangen.de/resources/MIR/FMP/data/C1/FMP_C1_F13.png" height="400">
							<p style="font-size:14px; margin: 0; "><em>Retrieved From: <a href="https://www.audiolabs-erlangen.de/">Audiolabs-Erlangen [4]</a></em></p>
						</div>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape style="font-size: 18px;">
								[[note on/off, pitch, ticks], [1, 67, 60], [1, 55, 0], [1, 43, 0], [0, 67, 55], ...]
							</code>
						</pre>
					</section>

					<section>
						<h3>GAN Model Example</h3>
						<audio controls src="static/gan.mp3" type="audio/mpeg"></audio>
					</section>

					<!-- Transformer Model -->
					<section>
						<h3>Transformer Model</h3>
						<p style="font-size: 22px">The Transformer Model utilises sequence-to-sequence (notes and chords) techniques to predict the next sequence of notes.</p>
						<img style="filter: invert(100);" class="fragment zoom-in" src="static/transformer.png">
					</section>
					<section>
						<h4>Transformer Model (Seq2Seq Process)</h4>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape>
								['B4', 'A4', 'A2', 'E3', '0.4', '10.0', '0.5', '0.4.7', ...]
							</code>
						</pre>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape>
								[1, 2, 3, 4, 5, 6, 7, 8, ...]
							</code>
						</pre>
						<i class="fas fa-arrow-down fragment zoom-in"></i>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape>
								[9, 10, 11, 12, 13, 14, 15, 16, ...]
							</code>
						</pre>
						<pre class="fragment zoom-in">
							<code data-trim data-noescape>
								['B2', 'A1', 'A3', 'E2', '0.7', '5.3', '5.9', '1.3.10', ...]
							</code>
						</pre>
					</section>

					<section>
						<h3>Transformer Model Example</h3>
						<audio controls src="static/transformer.mp3" type="audio/mpeg"></audio>
					</section>

					<section>
						<h3>Source Code</h3>
						<a href="https://github.com/ajbuchholz/CSC3094-Music-GAN-Dissertation"><img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" style="filter: invert(100); width: 30%;"></a>
					</section>
				</section>

				<!-- Future Plans -->
				<section>
					<h3>Future Plans</h3>
					<div class="r-stack">
						<blockquote class="fragment zoom-in" style="width:auto; margin: 12px; padding: 24px; font-size: 26px;">
							<ul class="fragment fade-in" ="font-size: 20px;">
								<li>Modify the GAN model to work with multiple instruments.</li>
								<li>Continue working on the dissertation writeup (Currently at ~10,000 words).</li>
								<li>Finalise poster and submit before the end of spring break.</li>
							</ul>
						</blockquote>
					</div>
				</section>

				<section>
					<h3>Summary</h3>
					<div class="r-stack">
						<blockquote class="fragment zoom-in" style="width:auto; margin: 12px; padding: 24px; font-size: 26px;">
							<ul class="fragment fade-in" ="font-size: 20px;">
								<li>Finished the RNN Models (RNN-Pitch & Chord)</li>
								<li>Finished the GAN Model</li>
								<li>Finished the Transformer Model</li>
								<li>Began writing the paper</li>
							</ul>
						<audio class="fragment zoom-in" controls src="static/rnn-chord-best.mp3" type="audio/mpeg"></audio>
						</blockquote>
					</div>
				</section>

				<!-- References -->
				<section>
					<h3>References</h3>
					<p style="font-size: 16px; text-align: left;">
						[1] “Generate music with an RNN | TensorFlow Core,” TensorFlow. https://www.tensorflow.org/tutorials/audio/music_generation#setup.<br>
						[2] “Music Transformer: Generating Music with Long-Term Structure,” research.google. https://research.google/pubs/music-transformer-generating-music-with-long-term-structure/.<br>
						[3] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, “MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment,” arXiv.org, 2017. https://arxiv.org/abs/1709.06298.<br>
						[4] A. Labs, FMP_C1_F13, MIDI Note Event Representation. Available: https://www.audiolabs-erlangen.de/resources/MIR/FMP/data/C1/FMP_C1_F13.png<br>
					   	[5] B. Rechner, Tontechnik-Rechner, MIDI Note Names. Available: https://sengpielaudio.com/calculator-notenames<br>
					   	[6] D. Kulma, David Kulma, Pitch Class. Available: https://davidkulma.com/musictheory/integers<br>
					   	[7] M. Tits, ResearchGate, MIDI Note Numbers. Available: https://www.researchgate.net/figure/88-notes-classical-keyboard-Note-names-and-MIDI-numbers_fig8_283460243<br>
					   	[8] Builtin.com, 2024. https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/2_waterfall%20methodology.png.				
					</p>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
